# -*- coding: utf-8 -*-
"""spike_sorting_with_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sXJbiqjxY_HSWMmMKCnXWvc7mM16RWZw
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from scipy.io import loadmat

from tensorflow.keras.layers import Input, Dense, Reshape, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras import losses, utils
from tensorflow.keras.models import Model

file = loadmat('/content/drive/MyDrive/spike_data.mat')
data = file['wf']

# training dataset
x_train = data[18][1]
x_train = np.append(x_train, data[18][2], axis=0)
x_train = np.append(x_train, data[18][4], axis=0)

y_train = np.ones(x_train.shape[0])
y_train[:data[18][1].shape[0]] = 0
y_train[data[18][1].shape[0]:data[18][2].shape[0]] = 1
y_train[data[18][2].shape[0]:] = 2
y_train = y_train.astype(np.int32)

# testing datatest
x_test = data[20][1]
x_test = np.append(x_test, data[20][2], axis=0)
x_test = np.append(x_test, data[20][3], axis=0)

y_test = np.ones(x_test.shape[0])
y_test[:data[20][1].shape[0]] = 0
y_test[data[20][1].shape[0]:data[20][2].shape[0]] = 1
y_test[data[20][2].shape[0]:] = 2
y_test = y_test.astype(np.int32)

def build():
  raw_data = Input(shape=(64,1))

  conv1 = Conv1D(filters=8, kernel_size=3, strides=1, padding='valid', activation='relu')(raw_data)
  pool1 = MaxPooling1D(pool_size=2, strides=1, padding='valid')(conv1)

  conv2 = Conv1D(filters=16, kernel_size=3, strides=1, padding='valid', activation='relu')(pool1)
  pool2 = MaxPooling1D(pool_size=2, strides=1, padding='valid')(conv2)

  conv3 = Conv1D(filters=16, kernel_size=3, strides=1, padding='valid', activation='relu')(pool2)
  pool3 = MaxPooling1D(pool_size=2, strides=1, padding='valid')(conv3)

  flattened = Flatten()(pool3)

  dense = Dense(units=32, activation='relu')(flattened)
  output = Dense(units=3, activation='softmax')(dense)

  model = Model(inputs=raw_data, outputs=output)
  return model

spk_sorting_cnn = build()
spk_sorting_cnn.compile(optimizer='adam', loss='SparseCategoricalCrossentropy', metrics=['accuracy'])
spk_sorting_cnn.summary()

h = spk_sorting_cnn.fit(x_train, y_train,
                batch_size=320,
                epochs=10,
                shuffle=True,
                validation_data=(x_test, y_test)
                )

y_predict = spk_sorting_cnn.predict(x_test)

'''y_result = np.amax(y_predict, axis=1)
print(y_predict)
print(y_result)'''

plt.plot(h.history['accuracy'])
plt.plot(h.history['val_accuracy'])
plt.title('model accuracy')
plt.xticks(range(1, 10))
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='center right')
plt.show()
#loss model
plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xticks(range(1, 10))
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='center right')
plt.show()