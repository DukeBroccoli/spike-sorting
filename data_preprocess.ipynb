{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvN1Ctxc1qHu"
      },
      "source": [
        "# concatenate data from different units (data in spike_data.mat are already sorted)\n",
        "def concat(*data_block):\n",
        "  data_concat = data_block[0]\n",
        "  for i in range(1,len(data_block)):\n",
        "    data_concat = np.vstack((data_concat, data_block[i]))\n",
        "  return data_concat\n",
        "\n",
        "# remove all outliers from dataset\n",
        "def remove_outlier(data):\n",
        "  # compute zscore\n",
        "  z = np.abs(stats.zscore(data))\n",
        "  outlier_row_idx = np.where(z>3)[0]\n",
        "\n",
        "  # remove all repeated elements in rows_to_rm_idx\n",
        "  temp = outlier_row_idx.shape[0]\n",
        "  for i in range(temp):\n",
        "    if i<temp-1 and outlier_row_idx[i]==outlier_row_idx[i+1]:\n",
        "      outlier_row_idx = np.delete(outlier_row_idx, i)\n",
        "      temp-=1\n",
        "\n",
        "  # remove all rows containing outliers\n",
        "  data_new = np.zeros((1, data.shape[1]), dtype='float32')\n",
        "  for i in range(data.shape[0]):\n",
        "    if not np.isin(i, outlier_row_idx):\n",
        "      row_temp = np.reshape(data[i], (1, data.shape[1]))\n",
        "      data_new = np.append(data_new, row_temp, axis=0)\n",
        "  data_new = np.delete(data_new, 0, 0)\n",
        "\n",
        "  return data_new\n",
        "\n",
        "# different ways to pre-process data\n",
        "def centralize(data):\n",
        "  return data - tf.reduce_mean(data, axis=0, keepdims=True)\n",
        "  \n",
        "def scale(data):\n",
        "  return MinMaxScaler().fit_transform(data)\n",
        "\n",
        "def standardize(data):\n",
        "  return StandardScaler().fit_transform(data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}